{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73af780",
   "metadata": {},
   "source": [
    "# üöÄ Scikit-Learn Phase 1: ML Basics & First Models\n",
    "\n",
    "**Your next step after NumPy, Pandas & Matplotlib**\n",
    "\n",
    "In this notebook you will learn:\n",
    "1. Data Preprocessing (missing values, encoding)\n",
    "2. Feature Scaling & Normalization\n",
    "3. Train-Test Splitting\n",
    "4. Linear Regression (Regression)\n",
    "5. Logistic Regression (Classification)\n",
    "6. Decision Tree Classifier\n",
    "7. Model Evaluation Metrics\n",
    "8. Cross-Validation\n",
    "9. Hyperparameter Tuning (GridSearchCV)\n",
    "10. ML Pipelines\n",
    "11. Saving & Loading Models\n",
    "\n",
    "---\n",
    "\n",
    "### The ML Workflow\n",
    "```\n",
    "Load Data ‚Üí Preprocess ‚Üí Split ‚Üí Train ‚Üí Evaluate ‚Üí Predict ‚Üí Deploy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9d7b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " Great question ‚Äî you're right to be skeptical. Some of those StatQuest/Krish Naik videos are **10-20 min conceptual explainers**, which are great for intuition but **not enough alone** for hands-on learning. Here's the fix:\n",
    "\n",
    "## Updated Strategy: Use **Long-Form Tutorials** as Primary + **Short Videos** as Supplements\n",
    "\n",
    "### Best Long-Form Resources (Hindi + English)\n",
    "\n",
    "| Resource | Duration | Why it works |\n",
    "|----------|----------|-------------|\n",
    "| **CampusX ML Playlist** | 1.5-2 hrs/video | Deep theory + code. Best for your level. [Full Playlist](https://www.youtube.com/playlist?list=PLKnIA16OIAFMLe-8kQm4SavhGln4TgaEd) |\n",
    "| **Krish Naik Complete ML Playlist** | 30-60 min/video | Practical, project-oriented. [Full Playlist](https://www.youtube.com/playlist?list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe) |\n",
    "| **Codebasics ML Playlist** | 15-30 min/video | Short but code-heavy, good for coding along. [Full Playlist](https://www.youtube.com/playlist?list=PLeo1K3hjS3uvCeTRe6-LeyiOtwM9PUZO0) |\n",
    "| **StatQuest** | 8-20 min | **Use ONLY as a supplement** to understand the math/intuition visually |\n",
    "\n",
    "### Revised 30-Day Plan (Longer, More Comprehensive Videos)\n",
    "\n",
    "#### Week 1: Regression\n",
    "| Day | Topic | Primary Video (Long) | Supplement (Short) |\n",
    "|-----|-------|---------------------|-------------------|\n",
    "| 1 | Finish Preprocessing | [CampusX - Preprocessing (2hr)](https://www.youtube.com/watch?v=RiEpSd4j0vE) | ‚Äî |\n",
    "| 2 | Linear Regression Theory + Code | [CampusX - Linear Regression (1.5hr)](https://www.youtube.com/watch?v=UZPfbG0jNec) | [StatQuest - Linear Regression (12min)](https://www.youtube.com/watch?v=PaFPbb66DxQ) |\n",
    "| 3 | Gradient Descent Deep Dive | [CampusX - Gradient Descent (1.5hr)](https://www.youtube.com/watch?v=ORyfPJypKuU) | [3Blue1Brown - Gradient Descent (21min)](https://www.youtube.com/watch?v=IHZwWFHWa-w) |\n",
    "| 4 | Multiple + Polynomial Regression | [CampusX - Multiple LR (1.5hr)](https://www.youtube.com/watch?v=2eYWJIBRSGc) | ‚Äî |\n",
    "| 5 | Ridge, Lasso, ElasticNet | [CampusX - Regularization (1.5hr)](https://www.youtube.com/watch?v=aEow2V8jEyE) | [StatQuest - Ridge (21min)](https://www.youtube.com/watch?v=Q81RR3yKn30) |\n",
    "| 6-7 | **Project: House Price Prediction** | [Krish Naik - End to End ML (1hr)](https://www.youtube.com/watch?v=p1hGz0w_OCo) | Code along in your notebook |\n",
    "\n",
    "#### Week 2: Classification\n",
    "| Day | Topic | Primary Video (Long) | Supplement |\n",
    "|-----|-------|---------------------|-----------|\n",
    "| 8 | Logistic Regression | [CampusX - Logistic Regression (2hr)](https://www.youtube.com/watch?v=ABhFGsEGIKA) | [StatQuest (9min)](https://www.youtube.com/watch?v=yIYKR4sgzI8) |\n",
    "| 9 | Decision Trees | [CampusX - Decision Tree (1.5hr)](https://www.youtube.com/watch?v=PHxYNGo8NcI) | [StatQuest (18min)](https://www.youtube.com/watch?v=_L39rN6gz7Y) |\n",
    "| 10 | Random Forest + Bagging | [CampusX - Random Forest (1.5hr)](https://www.youtube.com/watch?v=bHK1nDNRHO0) | [StatQuest (10min)](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ) |\n",
    "| 11 | SVM | [CampusX - SVM (2hr)](https://www.youtube.com/watch?v=ugTxMLjLS8M) | ‚Äî |\n",
    "| 12 | KNN + Naive Bayes | [CampusX - KNN (1.5hr)](https://www.youtube.com/watch?v=BYaOETcsMaA) | ‚Äî |\n",
    "| 13 | Model Evaluation Metrics | [CampusX - Metrics (1.5hr)](https://www.youtube.com/watch?v=EUiIydNBIbE) | [StatQuest - Confusion Matrix (7min)](https://www.youtube.com/watch?v=Kdsp6soqA7o) |\n",
    "| 14 | **Project: Classification Project** | [Krish Naik - Heart Disease (45min)](https://www.youtube.com/watch?v=fHFOANOPMng) | Code along |\n",
    "\n",
    "#### Week 3: Unsupervised + Boosting\n",
    "| Day | Topic | Primary Video (Long) |\n",
    "|-----|-------|---------------------|\n",
    "| 15 | K-Means Clustering | [CampusX - KMeans (1.5hr)](https://www.youtube.com/watch?v=5shTLzwAdEc) |\n",
    "| 16 | Hierarchical + DBSCAN | [CampusX - DBSCAN (1.5hr)](https://www.youtube.com/watch?v=DQabDWPqWiE) |\n",
    "| 17 | PCA | [CampusX - PCA (2hr)](https://www.youtube.com/watch?v=SJpE2_YEb-8) |\n",
    "| 18 | Feature Engineering | [CampusX - Feature Eng (2hr)](https://www.youtube.com/watch?v=6WDFfaYtN6s) |\n",
    "| 19 | XGBoost / AdaBoost | [CampusX - Boosting (1.5hr)](https://www.youtube.com/watch?v=Tw05ywNJMaQ) |\n",
    "| 20-21 | **Project: Full ML Pipeline** | [Krish Naik - Full Project (1hr)](https://www.youtube.com/watch?v=fHFOANOPMng) |\n",
    "\n",
    "#### Week 4: Deep Learning Intro\n",
    "| Day | Topic | Primary Video (Long) |\n",
    "|-----|-------|---------------------|\n",
    "| 22-23 | Neural Network Intuition + Math | [3Blue1Brown - NN Series (4 videos, ~1hr total)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) |\n",
    "| 24-25 | TensorFlow/Keras Hands-on | [Codebasics - DL Playlist (start)](https://www.youtube.com/playlist?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDR1) |\n",
    "| 26-27 | CNN Basics | [CampusX - CNN](https://www.youtube.com/watch?v=zfiSAzpy9NM) or Codebasics CNN video |\n",
    "| 28-29 | NLP Basics | [CampusX - NLP Intro (2hr)](https://www.youtube.com/watch?v=6ZHlThRqO2g) |\n",
    "| 30 | Portfolio Review + Deployment | [Krish Naik - ML Deployment (1hr)](https://www.youtube.com/watch?v=bjsJOl8gz5k) |\n",
    "\n",
    "---\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "- **CampusX = your main teacher** (long, thorough, code-along, Hindi)\n",
    "- **StatQuest = watch BEFORE CampusX** for 10-min visual intuition on the topic\n",
    "- **Krish Naik / Codebasics = project days** for building end-to-end\n",
    "- **3Blue1Brown = only for neural networks** math intuition\n",
    "\n",
    "Daily time: **~2-3 hours** (one long video + coding along in your notebooks). The short 6-min videos are just appetizers ‚Äî the CampusX sessions are the real meal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c6e3c",
   "metadata": {},
   "source": [
    "## Section 1: Data Preprocessing with Scikit-Learn\n",
    "\n",
    "Before feeding data to any ML model, we need to **clean and prepare** it:\n",
    "- Handle **missing values** ‚Üí `SimpleImputer`\n",
    "- Encode **categorical variables** ‚Üí `LabelEncoder`, `OneHotEncoder`\n",
    "- These are found in `sklearn.preprocessing` and `sklearn.impute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import all libraries we'll use throughout this notebook ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn modules\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,\n",
    "                             mean_squared_error, r2_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Handling Missing Values with SimpleImputer ---\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = pd.DataFrame({\n",
    "    'Age':    [25, np.nan, 35, 40, np.nan, 30],\n",
    "    'Salary': [50000, 60000, np.nan, 80000, 70000, np.nan],\n",
    "    'City':   ['Delhi', 'Mumbai', 'Delhi', np.nan, 'Mumbai', 'Pune']\n",
    "})\n",
    "\n",
    "print(\"üìå Original Data:\")\n",
    "print(data)\n",
    "print(f\"\\nMissing values:\\n{data.isnull().sum()}\")\n",
    "\n",
    "# Impute numerical columns with MEAN\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "data[['Age', 'Salary']] = num_imputer.fit_transform(data[['Age', 'Salary']])\n",
    "\n",
    "# Impute categorical column with MOST FREQUENT value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[['City']] = cat_imputer.fit_transform(data[['City']])\n",
    "\n",
    "print(\"\\n‚úÖ After Imputation:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da29bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encoding Categorical Variables ---\n",
    "\n",
    "# LabelEncoder: converts categories to numbers (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "data['City_encoded'] = le.fit_transform(data['City'])\n",
    "print(\"LabelEncoder mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print(data[['City', 'City_encoded']])\n",
    "\n",
    "# OneHotEncoder: creates binary columns for each category (better for ML)\n",
    "print(\"\\n--- OneHotEncoding ---\")\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids dummy variable trap\n",
    "city_ohe = ohe.fit_transform(data[['City']])\n",
    "ohe_df = pd.DataFrame(city_ohe, columns=ohe.get_feature_names_out(['City']))\n",
    "print(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf124b08",
   "metadata": {},
   "source": [
    "## Section 2: Feature Scaling & Normalization\n",
    "\n",
    "ML algorithms like Logistic Regression, KNN, SVM are **distance-based** ‚Äî they perform badly when features have very different scales.\n",
    "\n",
    "| Technique | Formula | Range | When to use |\n",
    "|-----------|---------|-------|-------------|\n",
    "| **StandardScaler** | (x - mean) / std | ~ -3 to +3 | Most algorithms |\n",
    "| **MinMaxScaler** | (x - min) / (max - min) | 0 to 1 | Neural networks, images |\n",
    "\n",
    "‚ö†Ô∏è **Always fit on training data, then transform both train & test!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling Demo ---\n",
    "\n",
    "# Sample data with different scales\n",
    "sample = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'Salary': [30000, 45000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "})\n",
    "\n",
    "# StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "scaled_std = scaler_std.fit_transform(sample)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_mm = MinMaxScaler()\n",
    "scaled_mm = scaler_mm.fit_transform(sample)\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].hist(sample['Age'], alpha=0.7, label='Age', color='steelblue')\n",
    "axes[0].hist(sample['Salary'], alpha=0.7, label='Salary', color='coral')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"StandardScaler (mean=0, std=1)\")\n",
    "axes[1].hist(scaled_std[:, 0], alpha=0.7, label='Age', color='steelblue')\n",
    "axes[1].hist(scaled_std[:, 1], alpha=0.7, label='Salary', color='coral')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_title(\"MinMaxScaler (0 to 1)\")\n",
    "axes[2].hist(scaled_mm[:, 0], alpha=0.7, label='Age', color='steelblue')\n",
    "axes[2].hist(scaled_mm[:, 1], alpha=0.7, label='Salary', color='coral')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Before scaling ‚Äî Age mean: {sample['Age'].mean():.1f}, Salary mean: {sample['Salary'].mean():.1f}\")\n",
    "print(f\"After StandardScaler ‚Äî Age mean: {scaled_std[:,0].mean():.4f}, Salary mean: {scaled_std[:,1].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1af881",
   "metadata": {},
   "source": [
    "## Section 3: Splitting Data into Training & Testing Sets\n",
    "\n",
    "We **never** train and test on the same data ‚Äî that's like memorizing answers before an exam.\n",
    "\n",
    "- **Training set** (~80%): model learns from this\n",
    "- **Testing set** (~20%): model is evaluated on this (unseen data)\n",
    "\n",
    "Key parameters of `train_test_split`:\n",
    "- `test_size=0.2` ‚Üí 20% for testing\n",
    "- `random_state=42` ‚Üí reproducible results every time\n",
    "- `stratify=y` ‚Üí keeps class proportions same in both splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ecb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train-Test Split ---\n",
    "\n",
    "# Load the Iris dataset (built-in, no download needed)\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\\n\")\n",
    "\n",
    "# Split: 80% train, 20% test, stratified\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set:  {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b44272",
   "metadata": {},
   "source": [
    "## Section 4: Linear Regression (Your First Regression Model)\n",
    "\n",
    "**Regression** = predicting a continuous number (price, temperature, salary)\n",
    "\n",
    "Linear Regression finds the best straight line: **y = mx + b**\n",
    "\n",
    "We'll use the **California Housing** dataset (predicting house prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Linear Regression on California Housing ---\n",
    "\n",
    "housing = datasets.fetch_california_housing()\n",
    "X_h = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y_h = pd.Series(housing.target, name='MedHouseVal')\n",
    "\n",
    "print(\"Features:\", list(X_h.columns))\n",
    "print(f\"Shape: {X_h.shape}\")\n",
    "print(X_h.head())\n",
    "\n",
    "# Split\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_h, y_h, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_h, y_train_h)\n",
    "\n",
    "# Predict\n",
    "y_pred_h = lr.predict(X_test_h)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test_h, y_pred_h)\n",
    "r2 = r2_score(y_test_h, y_pred_h)\n",
    "print(f\"\\nüìä Linear Regression Results:\")\n",
    "print(f\"   Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"   R¬≤ Score:           {r2:.4f}  (1.0 = perfect)\")\n",
    "\n",
    "# Plot: Actual vs Predicted\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test_h, y_pred_h, alpha=0.3, s=10, color='steelblue')\n",
    "plt.plot([0, 5], [0, 5], 'r--', label='Perfect prediction')\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Linear Regression: Actual vs Predicted House Prices\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b93f5",
   "metadata": {},
   "source": [
    "## Section 5: Logistic Regression for Classification\n",
    "\n",
    "**Classification** = predicting a category (spam/not spam, cat/dog, disease/healthy)\n",
    "\n",
    "Despite the name, **Logistic Regression** is a **classification** algorithm:\n",
    "- Outputs probabilities (0 to 1) using the sigmoid function\n",
    "- Works great as a baseline classifier\n",
    "- We'll classify Iris flowers into 3 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fa0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression on Iris Dataset ---\n",
    "\n",
    "# Scale features first (important for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"‚úÖ Logistic Regression Accuracy: {acc_lr:.4f} ({acc_lr*100:.1f}%)\")\n",
    "print(f\"\\nüìã Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=iris.target_names))\n",
    "\n",
    "# Decision Boundary Visualization (using 2 features only)\n",
    "X_2d = X_train[['sepal length (cm)', 'sepal width (cm)']].values\n",
    "X_2d_scaled = StandardScaler().fit_transform(X_2d)\n",
    "lr_2d = LogisticRegression(max_iter=200, random_state=42)\n",
    "lr_2d.fit(X_2d_scaled, y_train)\n",
    "\n",
    "# Create mesh grid\n",
    "x_min, x_max = X_2d_scaled[:, 0].min() - 1, X_2d_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_2d_scaled[:, 1].min() - 1, X_2d_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "Z = lr_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "scatter = plt.scatter(X_2d_scaled[:, 0], X_2d_scaled[:, 1], c=y_train, cmap='viridis', edgecolors='k', s=40)\n",
    "plt.xlabel(\"Sepal Length (scaled)\")\n",
    "plt.ylabel(\"Sepal Width (scaled)\")\n",
    "plt.title(\"Logistic Regression ‚Äî Decision Boundary (2 features)\")\n",
    "plt.colorbar(scatter, label='Species')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65ebe0",
   "metadata": {},
   "source": [
    "## Section 6: Decision Tree Classifier\n",
    "\n",
    "Decision Trees split data by asking **yes/no questions** on features:\n",
    "- Easy to understand and visualize\n",
    "- No need for feature scaling\n",
    "- Can overfit if tree grows too deep\n",
    "\n",
    "Let's train one on the same Iris dataset and compare with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decision Tree Classifier ---\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)  # No scaling needed for trees!\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"‚úÖ Decision Tree Accuracy: {acc_dt:.4f} ({acc_dt*100:.1f}%)\")\n",
    "print(f\"\\nüìã Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=iris.target_names))\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\nüîç Comparison:\")\n",
    "print(f\"   Logistic Regression: {acc_lr*100:.1f}%\")\n",
    "print(f\"   Decision Tree:       {acc_dt*100:.1f}%\")\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(dt, feature_names=iris.feature_names, class_names=iris.target_names,\n",
    "          filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5391a",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation Metrics\n",
    "\n",
    "How do we know if a model is **actually good**?\n",
    "\n",
    "**Classification Metrics:**\n",
    "| Metric | What it measures |\n",
    "|--------|-----------------|\n",
    "| **Accuracy** | % of correct predictions |\n",
    "| **Precision** | Of all predicted positives, how many are actually positive? |\n",
    "| **Recall** | Of all actual positives, how many did we catch? |\n",
    "| **F1-Score** | Harmonic mean of Precision & Recall |\n",
    "| **Confusion Matrix** | Shows where the model is confused |\n",
    "\n",
    "**Regression Metrics:**\n",
    "| Metric | What it measures |\n",
    "|--------|-----------------|\n",
    "| **MSE** | Average squared error (lower = better) |\n",
    "| **RMSE** | Square root of MSE (same unit as target) |\n",
    "| **R¬≤ Score** | How much variance is explained (1.0 = perfect) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix Heatmap ---\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Logistic Regression ‚Äî Confusion Matrix')\n",
    "\n",
    "# Decision Tree confusion matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Decision Tree ‚Äî Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Regression metrics recap\n",
    "print(\"üìä Regression Metrics (California Housing ‚Äî from Section 4):\")\n",
    "print(f\"   MSE:  {mse:.4f}\")\n",
    "print(f\"   RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(f\"   R¬≤:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f94f2",
   "metadata": {},
   "source": [
    "## Section 8: Cross-Validation\n",
    "\n",
    "A **single** train/test split can be lucky or unlucky. **Cross-validation** tests the model on multiple splits:\n",
    "\n",
    "**K-Fold CV (k=5):**\n",
    "```\n",
    "Fold 1: [TEST] [train] [train] [train] [train]\n",
    "Fold 2: [train] [TEST] [train] [train] [train]\n",
    "Fold 3: [train] [train] [TEST] [train] [train]\n",
    "Fold 4: [train] [train] [train] [TEST] [train]\n",
    "Fold 5: [train] [train] [train] [train] [TEST]\n",
    "```\n",
    "Each fold gets a chance to be the test set ‚Üí more reliable score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Validation ---\n",
    "\n",
    "# Using a pipeline (scaler + model) to avoid data leakage\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=200, random_state=42))\n",
    "])\n",
    "\n",
    "pipe_dt = Pipeline([\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=3, random_state=42))\n",
    "])\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "cv_scores_lr = cross_val_score(pipe_lr, X, y, cv=5, scoring='accuracy')\n",
    "cv_scores_dt = cross_val_score(pipe_dt, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"üìä 5-Fold Cross-Validation Results:\")\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"   Fold scores: {cv_scores_lr}\")\n",
    "print(f\"   Mean: {cv_scores_lr.mean():.4f} ¬± {cv_scores_lr.std():.4f}\")\n",
    "\n",
    "print(f\"\\nDecision Tree:\")\n",
    "print(f\"   Fold scores: {cv_scores_dt}\")\n",
    "print(f\"   Mean: {cv_scores_dt.mean():.4f} ¬± {cv_scores_dt.std():.4f}\")\n",
    "\n",
    "# Plot fold scores\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "folds = range(1, 6)\n",
    "ax.plot(folds, cv_scores_lr, 'o-', label=f'Logistic Reg (mean={cv_scores_lr.mean():.3f})', color='steelblue')\n",
    "ax.plot(folds, cv_scores_dt, 's-', label=f'Decision Tree (mean={cv_scores_dt.mean():.3f})', color='coral')\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Cross-Validation Scores per Fold')\n",
    "ax.set_xticks(folds)\n",
    "ax.set_ylim(0.8, 1.05)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386f50",
   "metadata": {},
   "source": [
    "## Section 9: Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Models have **hyperparameters** (settings YOU choose before training). How to pick the best ones?\n",
    "\n",
    "**GridSearchCV** tries every combination and picks the best via cross-validation:\n",
    "```python\n",
    "param_grid = {'max_depth': [2, 3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
